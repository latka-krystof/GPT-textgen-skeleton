{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "from network import GPTLanguageModel\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hyperparameters from constants file\n",
    "batch_size = constants.BATCH_SIZE\n",
    "block_size = constants.BLOCK_SIZE\n",
    "num_epochs = constants.NUM_EPOCHS\n",
    "eval_interval = constants.EVAL_INTERVAL\n",
    "learning_rate = constants.LEARNING_RATE\n",
    "eval_iters = constants.EVAL_ITERS\n",
    "n_embd = constants.N_EMBD\n",
    "n_head = constants.N_HEAD\n",
    "n_layer = constants.N_LAYER\n",
    "dropout = constants.DROPOUT\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime config\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose what embeddings to use\n",
    "embeddings = None\n",
    "assert embeddings == 'gpt' or embeddings == 'character'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "data_input_dir = '../data/inputs/'\n",
    "output_dir = '../data/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed so that results can be replicated\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read the entire input file in as a single string\n",
    "text_data = None\n",
    "\n",
    "### TODO: Find the number of unique characters in the input text\n",
    "vocab = [] # obtain vocab from text_data\n",
    "char_vocab_size = len(vocab)\n",
    "print(\"There are %d unique characters in the data.\" % char_vocab_size)\n",
    "print(vocab)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that we have the unique characters in the text, we can create a mapping from characters to integers and integers to characters. This will allow us to encode the characters for processing in the network. Create two functions: one that maps characters to integers and one that maps integers to characters. This is the process of tokenization, in this case a simple mapping of the characters of the text to an integer. \n",
    "\"\"\"\n",
    "\n",
    "stoi = None # TODO: create a mapping from string (character) to integer\n",
    "itos = None # TODO: create a reverse mapping from integer to string (character)\n",
    "\n",
    "# save your mappings to retrieve them later for encoding/decoding\n",
    "np.save(f'{output_dir}stoi.npy', stoi)\n",
    "np.save(f'{output_dir}itos.npy', itos)\n",
    "\n",
    "encode = None # TODO: create a function that uses 'stoi' dictionary to encode a text of arbitrary length\n",
    "decode = None # TODO: create a function that uses 'itos' to decode text previously encoded \n",
    "\n",
    "# Encode the text_data using the stoi mapping\n",
    "basic_encoded_data = torch.tensor(encode(text_data), dtype=torch.long)\n",
    "print(\"Length of encoding:\", len(basic_encoded_data), \"Encoding:\", basic_encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "However, there are also other more complex forms of tokenization, which we encourage you to explore and experiment with on your own.\n",
    "Below, we present one example of a sub-word tokenizer package developed by OpenAI called tiktoken (full documentation at https://github.com/openai/tiktoken). If you have implemented the above cells correctly, you can see that the length of encoding for text_data using character-level encoding is much longer than the tiktoken (sub-word) encoding used by GPT-2. On the other hand, GPT-2 has a much larger vocab size (number of total available encodings), so we will need a larger token embedding table within the model architecture. \n",
    "\"\"\"\n",
    "# GPT embedding\n",
    "encoder = tiktoken.encoding_for_model('gpt2')\n",
    "gpt_encoded_data = torch.tensor(encoder.encode(text_data), dtype=torch.long)\n",
    "print(\"Length of encoding:\", len(gpt_encoded_data), \"Encoding:\", gpt_encoded_data)\n",
    "\n",
    "# default vocab size of GPT-2\n",
    "gpt_vocab_size = constants.GPT_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the data and vocab size corresponding to your chosen embedding option\n",
    "if embeddings == 'character':\n",
    "    encoded_data = basic_encoded_data\n",
    "    vocab_size = char_vocab_size\n",
    "else:\n",
    "    encoded_data = gpt_encoded_data\n",
    "    vocab_size = gpt_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform train-test split on encoded_data\n",
    "train_ratio = 0.0\n",
    "train_cutoff = None\n",
    "train_data = None\n",
    "test_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(split):\n",
    "    ### TODO: generate a small batch of data of inputs x and targets y\n",
    "    data = None # based on the value of split, load either training data or testing data\n",
    "    ix = None # generate a tensor of length batch_size that contains randomly generated valid indices within the data\n",
    "    x = None # for each index in ix, extract the corresponding chunk of text. Stack chunks using torch.stack()\n",
    "    y = None # stack targets (y) corresponding to inputs (x) from the previous line\n",
    "    x, y = x.to(device), y.to(device) # move data to GPU, if available\n",
    "    ###\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # switch mode to eval to disable gradient computation and compute loss faster\n",
    "    for split in ['train', 'test']:\n",
    "        losses = None # initialize a tensor of length eval_iters\n",
    "        for k in range(eval_iters):\n",
    "            # 1. Load a batch of data\n",
    "            # 2. Forward pass through your model to get logits and loss\n",
    "            # 3. Save your loss at a corresponding index in losses\n",
    "            pass\n",
    "        out[split] = losses.mean() # record mean loss for training and test set separately\n",
    "    model.train() # switch back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: choose a PyTorch optimizer for your model\n",
    "optimizer = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets, and save model parameters at this checkpoint\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, test loss {losses['test']:.4f}\")\n",
    "        # TODO: save model parameters \n",
    "\n",
    "    # TODO: sample a batch of training data\n",
    "    xb, yb = None, None\n",
    "\n",
    "    # TODO: Evaluate the loss and perform gradient step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
